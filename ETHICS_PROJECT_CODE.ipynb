{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aquisition Process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ashraq/financial-news-articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framing, Cleaning and Tokenizing the aquired data for usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def has_excessive_special_chars(sentence, max_ratio=0.15):\n",
    "    specials = re.findall(r'[^a-zA-Z0-9\\s]', sentence)\n",
    "    if not sentence.strip():\n",
    "        return True\n",
    "    ratio = len(specials) / len(sentence)\n",
    "    return ratio > max_ratio\n",
    "\n",
    "def filter_and_clean_token(token):\n",
    "    \"\"\"\n",
    "    1. Convert token to lowercase.\n",
    "    2. Remove specific special characters: (), /, \\\\, ~, `, \"\n",
    "    3. Strip leading and trailing punctuation.\n",
    "    4. Return the cleaned token if not empty, else None.\n",
    "    \"\"\"\n",
    "    token = token.lower()\n",
    "    # Remove specific special characters\n",
    "    token = re.sub(r'[()/\\\\~`\"]', '', token)\n",
    "    # Strip leading/trailing punctuation (any non-alphanumeric at edges)\n",
    "    token = re.sub(r'^\\W+|\\W+$', '', token)\n",
    "    return token if token else None\n",
    "\n",
    "def clean_text_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    1) Split 'text' into sentences\n",
    "    2) Filter out sentences with too many special chars\n",
    "    3) Tokenize each sentence & clean tokens\n",
    "    4) Reconstruct cleaned sentences and the full cleaned text\n",
    "    5) Return (cleaned_text, tokenized_sentences, tokenized_words)\n",
    "    \"\"\"\n",
    "    # 1) Sentence splitting\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    \n",
    "    # 2) Filter out invalid sentences\n",
    "    valid_sentences = [s for s in raw_sentences if not has_excessive_special_chars(s)]\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    # 3) Tokenize and clean each sentence\n",
    "    for s in valid_sentences:\n",
    "        tokens = word_tokenize(s)\n",
    "        cleaned_tokens = [filter_and_clean_token(t) for t in tokens]\n",
    "        # Remove None/empty tokens\n",
    "        cleaned_tokens = [ct for ct in cleaned_tokens if ct]\n",
    "        \n",
    "        if cleaned_tokens:\n",
    "            tokenized_sentences.append(cleaned_tokens)\n",
    "            # 4) Rebuild each cleaned sentence from cleaned_tokens\n",
    "            cleaned_sentences.append(\" \".join(cleaned_tokens))\n",
    "    \n",
    "    # Flatten tokens from all sentences\n",
    "    tokenized_words = [tok for sent in tokenized_sentences for tok in sent]\n",
    "    \n",
    "    # The full cleaned text = sentences joined by \". \" (or your preferred delimiter)\n",
    "    cleaned_text = \". \".join(cleaned_sentences)\n",
    "    \n",
    "    return cleaned_text, tokenized_sentences, tokenized_words\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply our unified function\n",
    "df[\"temp\"] = df[\"text\"].apply(clean_text_and_tokenize)\n",
    "\n",
    "# Split the tuple into three separate columns\n",
    "df[\"cleaned_text\"] = df[\"temp\"].apply(lambda x: x[0])\n",
    "df[\"tokenized_sentences\"] = df[\"temp\"].apply(lambda x: x[1])\n",
    "df[\"tokenized_words\"] = df[\"temp\"].apply(lambda x: x[2])\n",
    "\n",
    "# Drop the temp column\n",
    "df.drop(columns=[\"temp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"new_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *RUN THE **DATA_PREP FILE** AND GET THE ANNOTATED DATA*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the NER MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsUcuaVMJwht"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from seqeval.metrics import classification_report, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Entity map\n",
    "entity_map = {\n",
    "    \"O\": 0,\n",
    "    \"B-PER\": 1, \"I-PER\": 1,\n",
    "    \"B-LOC\": 2, \"I-LOC\": 2,\n",
    "    \"B-ORG\": 3, \"I-ORG\": 3,\n",
    "    \"B-MISC\": 6, \"I-MISC\": 6\n",
    "}\n",
    "\n",
    "df=pd.read_csv(\"ner_with_tags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35NUBaw3J3G_"
   },
   "outputs": [],
   "source": [
    "# Function to assign entity tags to tokens\n",
    "def process_row(row):\n",
    "    tokens = ast.literal_eval(row[\"tokenized_words\"])\n",
    "    tagged = ast.literal_eval(row[\"Tagged_POS\"])\n",
    "\n",
    "    # Flatten token list\n",
    "    if tokens and isinstance(tokens[0], list):\n",
    "        tokens = tokens[0]\n",
    "\n",
    "    # Initialize all tokens as 'O'\n",
    "    ner_tags = [0] * len(tokens)\n",
    "    text = row[\"cleaned_text\"]\n",
    "    token_offsets = []\n",
    "\n",
    "    # Build token start/end indices\n",
    "    cursor = 0\n",
    "    for token in tokens:\n",
    "        # Find the next occurrence of the token in the text\n",
    "        match = re.search(re.escape(token), text[cursor:])\n",
    "        if match:\n",
    "            start = cursor + match.start()\n",
    "            end = cursor + match.end()\n",
    "            token_offsets.append((start, end))\n",
    "            cursor = end\n",
    "        else:\n",
    "            token_offsets.append((cursor, cursor + len(token)))\n",
    "            cursor += len(token)\n",
    "\n",
    "    for ent in tagged:\n",
    "        group = ent.get(\"entity\", \"O\").upper()\n",
    "        entity_type = {\n",
    "            \"PER\": \"PER\",\n",
    "            \"LOC\": \"LOC\",\n",
    "            \"ORG\": \"ORG\",\n",
    "            \"MISC\": \"MISC\"\n",
    "        }.get(group, \"O\")\n",
    "\n",
    "        # Assign B- prefix for first token, I- for subsequent tokens\n",
    "        for i, (tok_start, tok_end) in enumerate(token_offsets):\n",
    "            if tok_end <= ent[\"start\"]:\n",
    "                continue\n",
    "            if tok_start >= ent[\"end\"]:\n",
    "                break\n",
    "            # Check if this is the first token of the entity\n",
    "            prefix = \"B-\" if (i == 0 or ner_tags[i-1] == 0) else \"I-\"\n",
    "            ner_tags[i] = entity_map.get(f\"{prefix}{entity_type}\", 0)\n",
    "\n",
    "    return {\"tokens\": tokens, \"ner_tags\": ner_tags}\n",
    "\n",
    "# Process entire dataset\n",
    "processed_data = [process_row(row) for _, row in df.iterrows()]\n",
    "dataset = Dataset.from_list(processed_data).train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416,
     "referenced_widgets": [
      "7e4362c09f704131835d740829c35f19",
      "fe1b0ca1bc7a486fae6950b8b6d1cf92",
      "536e5187c33044e594e4851e52a10866",
      "609bc6f691994c148fbaff78944decc0",
      "57a1c246ec28406b880ec18e2240ea3c",
      "cd13bad0f88d4e89b190362c3cd01e1c",
      "975c1ef26af2429f8020cd2c68db934a",
      "2a20c7a0811f4aec9a0e761e454997d4",
      "3a77cfe27a4e48b89e9df7187c144f02",
      "55bdc8a216164e08acf3360827bb8f82",
      "1106aee75ac643f7a606ae810f6dbea5",
      "68acfb1bc3e7451a84b3205a695c8397",
      "1b728c2217564e0991370b191486bff9",
      "0bf12077b2de481b86c32641df247e3f",
      "9087d6eebf774b378887bbf1a93a6c71",
      "1b7dfe75bd2e4f428de0feb9f364659b",
      "930e99a882b34addab60742d96933217",
      "7c8d7456281d4027af0c6e57044a13e1",
      "674397580ca54006ae19ca79b38a236d",
      "f00e5d9f11ba415db9b6292ca26badb5",
      "0451242645bf4c18bde7de9d50ffbf2f",
      "7aa776709dc24758bfd2c23fe2aff53d"
     ]
    },
    "id": "zYljFZfeJ6QR",
    "outputId": "8b67b6b4-bf18-4589-df13-e80c3014579e"
   },
   "outputs": [],
   "source": [
    "# Tokenizer & model\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(entity_map))\n",
    "\n",
    "# Tokenize & align\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, padding=\"max_length\", max_length=128, is_split_into_words=True)\n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(example[\"ner_tags\"][word_idx])\n",
    "        else:\n",
    "            label_ids.append(example[\"ner_tags\"][word_idx])\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "label_list = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'B-MISC', 'I-MISC']\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    for pred, label in zip(predictions, labels):\n",
    "        cur_preds = []\n",
    "        cur_labels = []\n",
    "        for p_i, l_i in zip(pred, label):\n",
    "            if l_i != -100:\n",
    "                cur_preds.append(label_list[p_i])\n",
    "                cur_labels.append(label_list[l_i])\n",
    "        true_predictions.append(cur_preds)\n",
    "        true_labels.append(cur_labels)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "        \"report\": classification_report(true_labels, true_predictions)\n",
    "    }\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./ner_model_temp\",         # where everything gets saved\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",            # saves checkpoint after each epoch\n",
    "    save_total_limit=2,               # keeps only last 2 checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"./logs\",             # TensorBoard logs\n",
    "    logging_strategy=\"epoch\",\n",
    "    report_to=\"none\",                 #'wandb' or 'tensorboard'\n",
    ")\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1MCV3fPPUtS"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f\"Results/{model_name}/eval_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BBVBERXxoPV",
    "outputId": "0b2893fc-5a94-44a1-94fd-b45c2477bf77"
   },
   "outputs": [],
   "source": [
    "print(metrics[\"eval_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "hWPbd0gpEld8",
    "outputId": "49f57be0-46e1-43a1-fc65-2098f9568ccc"
   },
   "outputs": [],
   "source": [
    "# Analyze the distribution of entity classes\n",
    "all_tags = [tag for example in processed_data for tag in example['ner_tags']]\n",
    "tag_counts = {label: all_tags.count(i) for i, label in enumerate(label_list)}\n",
    "\n",
    "# Filter out classes with zero counts\n",
    "filtered_tag_counts = {k: v for k, v in tag_counts.items() if v > 0}\n",
    "\n",
    "# Plot only non-zero classes\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.barplot(x=list(filtered_tag_counts.keys()), y=list(filtered_tag_counts.values()))\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribution of Entity Classes')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add exact counts on top of each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{int(p.get_height())}\",\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'Results/{model_name}/entity_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "ptc9r8vpFA4I",
    "outputId": "7e4ff7b6-109f-40f3-d26c-f814c3d44ebb"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions and labels\n",
    "predictions = trainer.predict(tokenized_dataset['test'])\n",
    "preds = np.argmax(predictions.predictions, axis=2)\n",
    "\n",
    "# Flatten predictions and labels (ignore -100)\n",
    "true_labels = []\n",
    "true_preds = []\n",
    "for pred, label in zip(preds, predictions.label_ids):\n",
    "    for p, l in zip(pred, label):\n",
    "        if l != -100:  # Skip padding tokens\n",
    "            true_preds.append(p)\n",
    "            true_labels.append(l)\n",
    "\n",
    "# --- Define whether to normalize ---\n",
    "NORMALIZE = False  # Set to False for raw counts\n",
    "\n",
    "# Get active classes (remove unused ones)\n",
    "active_classes = sorted(set(true_labels) | set(true_preds))\n",
    "filtered_label_list = [label_list[i] for i in active_classes]\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, true_preds, labels=active_classes)\n",
    "\n",
    "# Normalize if enabled\n",
    "if NORMALIZE:\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Row-wise normalization\n",
    "    fmt = '.2f'  # Show decimals for percentages\n",
    "else:\n",
    "    fmt = 'd'  # Show integers for raw counts\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=fmt,\n",
    "    xticklabels=filtered_label_list,\n",
    "    yticklabels=filtered_label_list,\n",
    "    cmap='Blues',\n",
    "    cbar=False if NORMALIZE else True  # Hide colorbar for normalized\n",
    ")\n",
    "plt.title(f\"{'Normalized' if NORMALIZE else 'Raw'} Confusion Matrix\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'Results/{model_name}/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "rqqyJBFXFCtG",
    "outputId": "9c0836f4-ed79-46b8-8087-2e1fcf220aff"
   },
   "outputs": [],
   "source": [
    "# Extract and visualize classification report\n",
    "report = metrics['eval_report']\n",
    "\n",
    "def parse_classification_report(report):\n",
    "    \"\"\"Parse classification report into a DataFrame\"\"\"\n",
    "    if isinstance(report, dict):\n",
    "        # Hugging Face's dictionary format\n",
    "        return pd.DataFrame(report).transpose()\n",
    "    else:\n",
    "        # String format (traditional scikit-learn style)\n",
    "        lines = [line.strip() for line in report.split('\\n') if line.strip()]\n",
    "        # Find the header line\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.startswith('precision'):\n",
    "                header = line.split()\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(\"Could not parse classification report\")\n",
    "\n",
    "        data = []\n",
    "        classes = []\n",
    "        for line in lines[i+1:-3]:  # Skip header and summary lines\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 5:  # Ensure we have all metrics\n",
    "                classes.append(parts[0])\n",
    "                data.append([float(x) for x in parts[1:4]])\n",
    "\n",
    "        return pd.DataFrame(data, index=classes, columns=header[:3])\n",
    "\n",
    "# Parse report\n",
    "try:\n",
    "    df_report = parse_classification_report(report)\n",
    "\n",
    "    # Filter out non-class rows (like 'accuracy', 'macro avg', 'weighted avg')\n",
    "    df_report = df_report.loc[~df_report.index.str.contains('avg|accuracy|total')]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    df_report[['precision', 'recall', 'f1-score']].plot(kind='bar')\n",
    "    plt.title('Precision, Recall, and F1-score per Entity Class')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1.1)  # Set consistent y-axis\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f'Results/{model_name}/classification_report.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error parsing report: {e}\")\n",
    "    print(\"Raw report content:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "id": "y33VpyZ_FErB",
    "outputId": "2ed8c636-3bcd-4afb-c394-a42db4f5f196"
   },
   "outputs": [],
   "source": [
    "# Identify most common misclassifications\n",
    "error_pairs = []\n",
    "for true, pred in zip(true_labels, true_preds):\n",
    "    if true != pred:\n",
    "        error_pairs.append((label_list[true], label_list[pred]))\n",
    "\n",
    "from collections import Counter\n",
    "common_errors = Counter(error_pairs).most_common(10)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=[f\"{true}→{pred}\" for (true, pred), count in common_errors],\n",
    "            y=[count for (true, pred), count in common_errors])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 10 Most Common Misclassifications')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.savefig(f'Results/{model_name}/most_common_errors.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "from IPython.display import display\n",
    "\n",
    "def visualize_ner(tokens, tags):\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "\n",
    "    for i, (token, tag_id) in enumerate(zip(tokens, tags)):\n",
    "        tag = label_list[tag_id]  # Get tag name (e.g., \"B-PER\")\n",
    "\n",
    "        # Skip O tags\n",
    "        if tag == 'O':\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "            continue\n",
    "\n",
    "        # Extract entity type (remove B-/I- prefix)\n",
    "        bio_prefix, entity_type = tag.split('-', 1) if '-' in tag else (None, tag)\n",
    "\n",
    "        # New entity starts with B- or when entity type changes\n",
    "        if bio_prefix == 'B' or (current_entity and current_entity[2] != entity_type):\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = [i, i+1, entity_type]  # [start, end, type]\n",
    "        # Continue existing entity\n",
    "        elif current_entity is not None:\n",
    "            current_entity[1] = i+1  # Update end position\n",
    "\n",
    "    # Add the last entity if exists\n",
    "    if current_entity is not None:\n",
    "        entities.append(current_entity)\n",
    "\n",
    "    # Format for displacy (with accurate token-based positions)\n",
    "    displacy_text = \" \".join(tokens)\n",
    "    displacy_ents = []\n",
    "    char_offset = 0\n",
    "\n",
    "    # Calculate character positions\n",
    "    for token, _ in zip(tokens, tags):\n",
    "        token_start = displacy_text.find(token, char_offset)\n",
    "        token_end = token_start + len(token)\n",
    "        char_offset = token_end + 1  # +1 for space\n",
    "\n",
    "    # Map entity token positions to character spans\n",
    "    char_pos = 0\n",
    "    token_positions = []\n",
    "    for token in tokens:\n",
    "        token_start = displacy_text.find(token, char_pos)\n",
    "        token_end = token_start + len(token)\n",
    "        token_positions.append((token_start, token_end))\n",
    "        char_pos = token_end + 1\n",
    "\n",
    "    # Create entities with correct character spans\n",
    "    displacy_ents = []\n",
    "    for start_idx, end_idx, label in entities:\n",
    "        char_start = token_positions[start_idx][0]\n",
    "        char_end = token_positions[end_idx-1][1]\n",
    "        displacy_ents.append({\n",
    "            \"start\": char_start,\n",
    "            \"end\": char_end,\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "\n",
    "    displacy_data = {\n",
    "        \"text\": displacy_text,\n",
    "        \"ents\": displacy_ents,\n",
    "        \"title\": None\n",
    "    }\n",
    "\n",
    "    display(displacy.render(\n",
    "        displacy_data,\n",
    "        style=\"ent\",\n",
    "        manual=True,\n",
    "    ))\n",
    "\n",
    "\n",
    "# Visualize 3 examples\n",
    "for i in range(87,90):\n",
    "    example = tokenized_dataset['test'][i]\n",
    "    tokens = example['tokens']\n",
    "    tags = [t if t != -100 else 0 for t in example['labels']]  # Convert -100 to O\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    visualize_ner(tokens, tags)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
